{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "Aortic Senosis (AS) is a degenerative valve condition leading to significant morbidity and mortality. AS is typically diagnosed by humans by reviewing transthoracic echocardiography, which produces images of the heart. Experts must parse through the images to find ones that contain a view of the aortic valve. Thus, the process of diagnosis is time consuming and error prone. The challenge in automating AS screening arises from the need for deep learning networks to mimic a human expert's ability to identify specific views of the aortic valve across those multiple images. Previous approaches to this problem have poor accuracy due to their inflexible averages across images, and normal attention-based multiple instance learning (MIL) models also perform poorly.\n",
        "\n",
        "\n",
        "The paper, `Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning` proposes a novel MIL-based model with two changes:\n",
        "- a supervised attention technique guides the learned attention mechanism to favor relevant views\n",
        "- a self-supervised pretraining strategy applies contrastive learning on the representation of the whole study as opposed to individual images\n",
        "\n",
        "The paper claims that their proposed approac (SAMIL) has better accuracy (by over 10%) in comparison to ABMIL.\n",
        "SABMIL, if this accurate in practice, has the potential to transform AS screening by making it less burdensome, more accurate, and more predictable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scope of Reproducibility\n",
        "\n",
        "The will test the following hypotheses from the paper:\n",
        "\n",
        "1. Supervised Attention Enhances Model Focus on Relevant Views: It hypothesizes that incorporating supervised attention within the MIL framework will significantly improve the model's ability to identify and prioritize echocardiogram views that are relevant for AS diagnosis. This mechanism is expected to lead to higher diagnostic accuracy by ensuring that the model's predictions are based on the most informative images.\n",
        "\n",
        "2. Self-Supervised Pretraining Improves Whole-Study Representation Learning: The hypothesis asserts that pretraining the model using a self-supervised strategy that focuses on whole- study representations rather than individual images will enhance the model's ability to understand and interpret the complex anatomical information presented in echocardiograms. This comprehensive learning approach is anticipated to yield a more accurate and robust model for AS screening.\n",
        "\n",
        "3. SAMIL Outperforms Conventional MIL Approaches in AS Detection: The proposal hypothesizes that the SAMIL framework, with its novel contributions of supervised attention and self-supervised whole-study pretraining, will outperform traditional MIL methods and existing AS detection models in terms of diagnostic accuracy and clinical relevance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m794myOt63Nv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "import PIL.Image as Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfk8Zrul_E8V",
        "outputId": "85246557-4d34-4fff-fdd0-f37333226e4a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1m0_UoU3m-o"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQkefVzzbUz_"
      },
      "source": [
        "### About TMED-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv6Y8hd6deLK"
      },
      "source": [
        "Our experiment uses Tufts Medical Echocardiogram Dataset-2 (TMED-2) dataset.\n",
        "  * TMED-2 is a clinically-motivated benchmark dataset for computer vision and machine learning from limited data.\n",
        "  * This dataset is designed to be an authentic assessment of semi-supervised learning methods that train classifiers from a small, hard-to-acquire labeled dataset and a much larger unlabeled set.\n",
        "  \n",
        "The TMED-2 dataset obtained from 2011-2020 at Tufts Medical Center.\n",
        "  * The TMED-2 dataset contains transthoracic echocardiogram (TTE) imagery, acquired in the course of routine care consistent with American Society of Echocardiography (ASE) guidelines.\n",
        "  * More information about retrieving the TMED-2 dataset can found [here](https://tmed.cs.tufts.edu/index.html).\n",
        "\n",
        "The access to the TMED-2 dataset was granted by requesting for the access on [here](https://tmed.cs.tufts.edu/index.html).\n",
        "  * Further modification wasn't required, and locating the retrieved folder was enough to process the experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcx0L_zYdUeo"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYOJbPUjdcJa"
      },
      "source": [
        "TMED-2 dataset contains three components:\n",
        "1. `view_and_diagnosis_labeled_set`: 599 bags from 577 unique patients, where all patients have an AS diagnostic label - (1)none, (2)early AS, or (3)significant AS. Also has view label annotations.\n",
        "      * This dataset has been partitioned by patient into 360 training / 119 validation / 120 test bags.\n",
        "2. `view_labeled_set`: 705 bags from 703 unique patients. These bags have view labels, but no AS diagnosis labels.\n",
        "3. `unlabeled_set`: 5,486 bags from 5,287 patients, and no labels are available for any bags in this set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frNHemfQc3wS"
      },
      "source": [
        "### Statistics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCuvcfOVdvUe"
      },
      "source": [
        "Below is the summary table of TMED-2 dataset:\n",
        "\n",
        "| Dataset            | Num. Patients | Num. Studies | Num. Labeled Images | Num. Unlabeled Images |\n",
        "|--------------------|---------------|--------------|---------------------|-----------------------|\n",
        "| fully labeled set  | 577           | 599          | 17,270               | 26,596                 |\n",
        "| partially labeled set | 703         | 705          | 7,694                | 37,576                 |\n",
        "| unlabeled set      | 5,287          | 5,486         | 0                   | 353,500                |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN7VyMph6Bjl"
      },
      "source": [
        "### Image Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXp9umfndrqL"
      },
      "source": [
        "Data processing has been already implemented on the dataset.\n",
        "* Every image in this dataset is a 2D transthoracic echocardiogram (TTE) image stored at 112x112 pixel resolution in PNG format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6PVOzW5dza_"
      },
      "source": [
        "### Illustrations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HDdOaO-7GOx"
      },
      "outputs": [],
      "source": [
        "def LoadImage(file_path):\n",
        "    im = Image.open(file_path)\n",
        "    im = np.asarray(im)/255\n",
        "    return im\n",
        "\n",
        "def ShowGrayscaleImage(file_path, title='', ax=None):\n",
        "    im = LoadImage(file_path)\n",
        "    if ax is None:\n",
        "        plt.figure()\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.imshow(im, cmap=plt.cm.gray, vmin=0, vmax=1)\n",
        "    plt.title(title)\n",
        "\n",
        "def find_label(query_key):\n",
        "    assert len(labels_per_image_csv.index[labels_per_image_csv['query_key']==query_key].tolist())==1\n",
        "\n",
        "    row_id = labels_per_image_csv.index[labels_per_image_csv['query_key']==query_key].tolist()[0]\n",
        "    view_label = labels_per_image_csv.iloc[row_id].view_label\n",
        "    diagnosis_label = labels_per_image_csv.iloc[row_id].diagnosis_label\n",
        "\n",
        "    return view_label, diagnosis_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6wJ3kHq79xa"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/CS598/TMED2/approved_users_only')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "M9ebcebb8CTO",
        "outputId": "2e82b3c4-e077-4d33-8b85-32ac15391c44"
      },
      "outputs": [],
      "source": [
        "labels_per_image_csv = pd.read_csv('labels_per_image.csv')\n",
        "labels_per_image_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "PnKMVSh08O3Q",
        "outputId": "4497ca18-f952-41ce-d167-6c37029e01cb"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "labels_per_image_csv.groupby('diagnosis_label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)\n",
        "plt.title('Diagnosis Label Distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "a6tu2plV8Kc-",
        "outputId": "0cfc98a9-1cf3-4957-c22c-826d4ddfffff"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "labels_per_image_csv.groupby('view_label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)\n",
        "plt.title('View Label Distribution')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3q1yPmn_dSY"
      },
      "source": [
        "**visualization from `view_and_diagnosis_labeled_set`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLRChNYe8xJy",
        "outputId": "6d861e6e-e094-4e69-b21d-09602bf66869"
      },
      "outputs": [],
      "source": [
        "ImageList_view_and_diagnosis_labeled_set = os.listdir('view_and_diagnosis_labeled_set/labeled 2')\n",
        "print('number of images in the ImageList_view_and_diagnosis_labeledset: {}'.format(len(ImageList_view_and_diagnosis_labeled_set)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFB9FOgN9Ny_"
      },
      "outputs": [],
      "source": [
        "im_to_visualize = random.choice(ImageList_view_and_diagnosis_labeled_set)\n",
        "view_label, diagnosis_label = find_label(im_to_visualize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "uo2Jw7X2_cCI",
        "outputId": "959f9d07-784f-43bc-9ec4-9292adb1bc0c"
      },
      "outputs": [],
      "source": [
        "print('Currently visualizing {} from labeled set'.format(im_to_visualize))\n",
        "print('view: {}, diagnosis: {}'.format(view_label, diagnosis_label))\n",
        "ShowGrayscaleImage(os.path.join('view_and_diagnosis_labeled_set/labeled 2', im_to_visualize))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvvR35Kg_hDg"
      },
      "source": [
        "**visualization `from view_labeled_set`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_cqOL7V_nVn",
        "outputId": "4ed2e3df-0249-49dc-b4ed-cf22313fa5e3"
      },
      "outputs": [],
      "source": [
        "ImageList_view_labeledset = os.listdir('view_labeled_set/labeled')\n",
        "print('number of images in the ImageList_view_labeledset: {}'.format(len(ImageList_view_labeledset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZezDRPPt_y7s"
      },
      "outputs": [],
      "source": [
        "im_to_visualize = random.choice(ImageList_view_labeledset)\n",
        "view_label, diagnosis_label = find_label(im_to_visualize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "BVNT1-m4_09g",
        "outputId": "d67fe611-0373-4ad6-ee16-8f0f59f44615"
      },
      "outputs": [],
      "source": [
        "print('Currently visualizing {} from labeled set'.format(im_to_visualize))\n",
        "print('view: {}, diagnosis: {}'.format(view_label, diagnosis_label))\n",
        "ShowGrayscaleImage(os.path.join('view_labeled_set/labeled', im_to_visualize))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
